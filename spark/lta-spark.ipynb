{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=TwitterStreamApp, master=local[*]) created by __init__ at <ipython-input-1-c70d4fc57c9c>:26 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c70d4fc57c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# create spark context with the above configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=TwitterStreamApp, master=local[*]) created by __init__ at <ipython-input-1-c70d4fc57c9c>:26 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext, SparkSession\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "TCP_REMOTE_HOST = \"lta-tweetgen.lta-net\" #docker container name of tweet generator\n",
    "TCP_PORT_INPUT = 9009 #must also be specified in the server file\n",
    "TCP_REMOTE_APPSERVER = 'lta-dashboard' # docker container name of app server\n",
    "TCP_PORT_OUTPUT = 9991 # must be exposed by app server docker container\n",
    "\n",
    "#define a dict of topics to track and count.\n",
    "#The keys are words that will be counted as a mention of the topic.\n",
    "#The values are the \"consolidated\" topics that will ultimately be tracked and charted.\n",
    "#All lowercase\n",
    "trackwords = {'trump' : 'trump',\n",
    "              'biden' : 'biden'}\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create the Streaming Context from the above \n",
    "#spark context with interval in sec\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"cps\")\n",
    "\n",
    "# read data from TCP\n",
    "dataStream = ssc.socketTextStream(TCP_REMOTE_HOST, TCP_PORT_INPUT)\n",
    "\n",
    "def send_to_app_server(rdd):\n",
    "    #translate down to arrays to send to server\n",
    "    arr = [x for x in rdd.toLocalIterator()]\n",
    "    #Array of the counts of mentions\n",
    "    counts = [y[1] for y in arr]\n",
    "    #Array of the labels in the RDD\n",
    "    topics = [y[0] for y in arr]\n",
    "    req_dict = dict(zip(topics, counts))\n",
    "    #add zeroes, if applicable\n",
    "    for _ in trackwords.values():\n",
    "        if _ not in req_dict.keys():\n",
    "            req_dict[_] = 0\n",
    "    url = 'http://{}:{}/updateData'.format(TCP_REMOTE_APPSERVER,TCP_PORT_OUTPUT)\n",
    "    response = requests.post(url, data=req_dict)\n",
    "\n",
    "#Parse the stream  such that each row is a list of length two\n",
    "#The UNIX timestamp of the date, and full text of the tweet\n",
    "parsedStream = dataStream.map(lambda line: [\n",
    "                        datetime.strptime(\n",
    "                            json.loads(line)['timestamp'],\n",
    "                            '%a %b %d %H:%M:%S +%f %Y').timestamp(),\n",
    "                        json.loads(line)['text'],\n",
    "                                            ])\n",
    "\n",
    "#tokenize the text of each tweet in the stream\n",
    "splitStream = parsedStream.map(lambda line:\n",
    "                         [line[0], line[1].lower().split(' ')])\n",
    "\n",
    "#filter stream to just the consolidated topics we want\n",
    "filteredStream = splitStream.map(lambda line: [line[0],\n",
    "                                   [trackwords[x] for x in line[1] if x in trackwords.keys()]])\n",
    "\n",
    "#itemize such that each token is it's own, timestamped row\n",
    "itemizedStream = filteredStream.flatMapValues(lambda _: _)\n",
    "\n",
    "\n",
    "#Count tokens by tag within each batch of the stream\n",
    "#This leaks the timestamp intentionally for now, though\n",
    "#It will be used in future expansions of this project\n",
    "tokensOnly = itemizedStream.map(lambda line: line[1])\n",
    "summedStream = tokensOnly.countByValue()\n",
    "\n",
    "#Send to a server for live plotting\n",
    "summedStream.foreachRDD(send_to_app_server)\n",
    "\n",
    "\n",
    "#Leave the below uncommented to see what is sent in the shell\n",
    "summedStream.pprint()\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# wait for the streaming to finish. This\n",
    "#never ends gracefully as of now\n",
    "ssc.awaitTermination()\n",
    "#https://issues.apache.org/jira/browse/SPARK-17397\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
